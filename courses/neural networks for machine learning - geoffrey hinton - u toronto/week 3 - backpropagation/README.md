# Week 3

[Link to Week in Coursera](https://www.coursera.org/learn/neural-networks/home/week/3)

## 3a - Linear Neuron Weight Learning

### Perceptron Learning Does Not Work For Hidden Layers

* every step, weights use averaging to get closer to "generously feasible" good weights
* networks where avg of two good solutions is a worse solution exist
* "multi-layer" NNs don't use perceptron 

### Alternative Learning Method Compares Target Output

* Perceptron shows weights get closer to good weights
* Alternative approach: show output values move closer to target
* Works for non-convex problems
  * where there exists several independent sets of good weights
  * where averaging good weights may give bad set of weights
* Simplest example: linear neuron with squared error

### Linear Neurons, AKA Linear Filters

### ![](/assets/linear-neurons-def1.png) {#linear-neuron-def1}

* ### 

## 3b - Linear Neuron Error Surface

## 3c - Logistic Output Neuron Weight Learning

## 3d - Backpropagation Algorithm

## 3e - Using Backpropagation Algorithm Derivatives



